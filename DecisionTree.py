# -*- coding: utf-8 -*-
"""AIMLsession12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Zs0QF2m4QE7rfjNuNqfawHdGp8YSZji
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import tree
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

clf = DecisionTreeClassifier(max_depth=5,criterion='entropy')
clf.fit(X_train,y_train)

y_pred = clf.predict(X_test)

print(accuracy_score(y_test,y_pred))

from sklearn.tree import export_text

tree_rules = export_text(clf,feature_names=iris.feature_names)
print(tree_rules)

plt.figure(figsize=(20,20))
tree.plot_tree(clf,filled=True,feature_names=iris.feature_names,class_names=iris.target_names)
plt.show()

clf.score(X_train,y_train)

clf.score(X_test,y_test)

node_impurities = clf.tree_.impurity

for i,entropy_calues in enumerate(node_impurities):
  print(f"Entropy for node {i}: {entropy_calues}")

from scipy.stats import entropy
from collections import Counter

def calculate_entropy(column):
    counts = Counter(column)
    probabilities = [count / len(column) for count in counts.values()]
    entropy_value = entropy(probabilities,base=2)
    return entropy_value

X = pd.DataFrame(iris.data,columns=iris.feature_names)

X.columns

for feature in X.columns:
  column_entropy = calculate_entropy(X[feature])
  print(f"Entropy for {feature}: {column_entropy}")

